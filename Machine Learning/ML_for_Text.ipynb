{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Initialization\" data-toc-modified-id=\"Initialization-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Initialization</a></span></li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Load Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#EDA\" data-toc-modified-id=\"EDA-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>EDA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#Evaluation-Procedure\" data-toc-modified-id=\"Evaluation-Procedure-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Evaluation Procedure</a></span></li><li><span><a href=\"#Normalization\" data-toc-modified-id=\"Normalization-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Normalization</a></span></li><li><span><a href=\"#Train-/-Test-Split\" data-toc-modified-id=\"Train-/-Test-Split-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Train / Test Split</a></span></li><li><span><a href=\"#Working-with-models\" data-toc-modified-id=\"Working-with-models-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Working with models</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-0---Constant\" data-toc-modified-id=\"Model-0---Constant-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Model 0 - Constant</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Model-1---NLTK,-TF-IDF-and-LR\" data-toc-modified-id=\"Model-1---NLTK,-TF-IDF-and-LR-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Model 1 - NLTK, TF-IDF and LR</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Model-3---spaCy,-TF-IDF-and-LR\" data-toc-modified-id=\"Model-3---spaCy,-TF-IDF-and-LR-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Model 3 - spaCy, TF-IDF and LR</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Model-4---spaCy,-TF-IDF-and-LGBMClassifier\" data-toc-modified-id=\"Model-4---spaCy,-TF-IDF-and-LGBMClassifier-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Model 4 - spaCy, TF-IDF and LGBMClassifier</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Model-9---BERT\" data-toc-modified-id=\"Model-9---BERT-7.9\"><span class=\"toc-item-num\">7.9&nbsp;&nbsp;</span>Model 9 - BERT</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7.10\"><span class=\"toc-item-num\">7.10&nbsp;&nbsp;</span>Conclusion</a></span></li></ul></li><li><span><a href=\"#My-Reviews\" data-toc-modified-id=\"My-Reviews-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>My Reviews</a></span><ul class=\"toc-item\"><li><span><a href=\"#Model-2\" data-toc-modified-id=\"Model-2-8.1\"><span class=\"toc-item-num\">8.1&nbsp;&nbsp;</span>Model 2</a></span></li><li><span><a href=\"#Model-3\" data-toc-modified-id=\"Model-3-8.2\"><span class=\"toc-item-num\">8.2&nbsp;&nbsp;</span>Model 3</a></span></li><li><span><a href=\"#Model-4\" data-toc-modified-id=\"Model-4-8.3\"><span class=\"toc-item-num\">8.3&nbsp;&nbsp;</span>Model 4</a></span></li><li><span><a href=\"#Model-9\" data-toc-modified-id=\"Model-9-8.4\"><span class=\"toc-item-num\">8.4&nbsp;&nbsp;</span>Model 9</a></span></li></ul></li><li><span><a href=\"#Final-Conclusion\" data-toc-modified-id=\"Final-Conclusion-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Final Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Film Junky Union, a new edgy community for classic movie enthusiasts, is developing a system for filtering and categorizing movie reviews. The goal is to train a model to automatically detect negative reviews. You'll be using a dataset of IMBD movie reviews with polarity labelling to build a model for classifying positive and negative reviews. It will need to have an F1 score of at least 0.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "# the next line provides graphs of better quality on HiDPI screens\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_csv('/datasets/imdb_reviews.tsv', sep='\\t', dtype={'votes': 'Int64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_reviews.info())\n",
    "display(df_reviews.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_reviews.isnull().sum().sum())\n",
    "\n",
    "df_reviews=df_reviews.dropna()\n",
    "\n",
    "display(df_reviews.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The functions describe() and info() was used to gather understanding of the dataset and the data types in the dataset. There is no extreme outliers in the data. The function isnull() was used to check for missing values. There were 4 missing values from 2 categories. These were dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of movies and reviews over years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "ax = axs[0]\n",
    "\n",
    "dft1 = df_reviews[['tconst', 'start_year']].drop_duplicates() \\\n",
    "    ['start_year'].value_counts().sort_index()\n",
    "dft1 = dft1.reindex(index=np.arange(dft1.index.min(), max(dft1.index.max(), 2021))).fillna(0)\n",
    "dft1.plot(kind='bar', ax=ax)\n",
    "ax.set_title('Number of Movies Over Years')\n",
    "\n",
    "ax = axs[1]\n",
    "\n",
    "dft2 = df_reviews.groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
    "\n",
    "dft2.plot(kind='bar', stacked=True, label='#reviews (neg, pos)', ax=ax)\n",
    "\n",
    "dft2 = df_reviews['start_year'].value_counts().sort_index()\n",
    "dft2 = dft2.reindex(index=np.arange(dft2.index.min(), max(dft2.index.max(), 2021))).fillna(0)\n",
    "dft3 = (dft2/dft1).fillna(0)\n",
    "axt = ax.twinx()\n",
    "dft3.reset_index(drop=True).rolling(5).mean().plot(color='orange', label='reviews per movie (avg over 5 years)', ax=axt)\n",
    "\n",
    "lines, labels = axt.get_legend_handles_labels()\n",
    "ax.legend(lines, labels, loc='upper left')\n",
    "\n",
    "ax.set_title('Number of Reviews Over Years')\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As the first figure shows the number of movies over the years has had a steep increase from 1990 onwards. This peaked in 2007.\n",
    "This could be a result of new technology giving the film industry the ability to make films quicker than before.\n",
    "As the second figure shows, there is a subsequent increase in the number of movie reviews, reaching the peak in 2007."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the distribution of number of reviews per movie with the exact counting and KDE (just to learn how it may differ from the exact counting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "ax = axs[0]\n",
    "dft = df_reviews.groupby('tconst')['review'].count() \\\n",
    "    .value_counts() \\\n",
    "    .sort_index()\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_title('Bar Plot of #Reviews Per Movie')\n",
    "\n",
    "ax = axs[1]\n",
    "dft = df_reviews.groupby('tconst')['review'].count()\n",
    "sns.kdeplot(dft, ax=ax)\n",
    "ax.set_title('KDE Plot of #Reviews Per Movie')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['pos'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax = axs[0]\n",
    "dft = df_reviews.query('ds_part == \"train\"')['rating'].value_counts().sort_index()\n",
    "dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_ylim([0, 5000])\n",
    "ax.set_title('The train set: distribution of ratings')\n",
    "\n",
    "ax = axs[1]\n",
    "dft = df_reviews.query('ds_part == \"test\"')['rating'].value_counts().sort_index()\n",
    "dft = dft.reindex(index=np.arange(min(dft.index.min(), 1), max(dft.index.max(), 11))).fillna(0)\n",
    "dft.plot.bar(ax=ax)\n",
    "ax.set_ylim([0, 5000])\n",
    "ax.set_title('The test set: distribution of ratings')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of negative and positive reviews over the years for two parts of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1)))\n",
    "\n",
    "ax = axs[0][0]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"train\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft.index = dft.index.astype('int')\n",
    "dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
    "dft.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('The train set: number of reviews of different polarities per year')\n",
    "\n",
    "ax = axs[0][1]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"train\"').groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)\n",
    "sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('The train set: distribution of different polarities per movie')\n",
    "\n",
    "ax = axs[1][0]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"test\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "dft.index = dft.index.astype('int')\n",
    "dft = dft.reindex(index=np.arange(dft.index.min(), max(dft.index.max(), 2020))).fillna(0)\n",
    "dft.plot(kind='bar', stacked=True, ax=ax)\n",
    "ax.set_title('The test set: number of reviews of different polarities per year')\n",
    "\n",
    "ax = axs[1][1]\n",
    "\n",
    "dft = df_reviews.query('ds_part == \"test\"').groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(dft[0], color='blue', label='negative', kernel='epa', ax=ax)\n",
    "sns.kdeplot(dft[1], color='green', label='positive', kernel='epa', ax=ax)\n",
    "ax.legend()\n",
    "ax.set_title('The test set: distribution of different polarities per movie')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The training and test set follow a similar distribution on the different polarities per movie. There is a peak of negative\n",
    "reviews at around 3 and this steeply declines to where at 4.5 onwards the distribution of positive reviews outweigh the \n",
    "negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Composing an evaluation routine which can be used for all models in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def evaluate_model(model, train_features, train_target, test_features, test_target):\n",
    "    \n",
    "    eval_stats = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6)) \n",
    "    \n",
    "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
    "        \n",
    "        eval_stats[type] = {}\n",
    "    \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)    \n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "\n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        aps = metrics.average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        if type == 'train':\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'green'\n",
    "\n",
    "        # F1 Score\n",
    "        ax = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('threshold')\n",
    "        ax.set_ylabel('F1')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'F1 Score') \n",
    "\n",
    "        # ROC\n",
    "        ax = axs[1]    \n",
    "        ax.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('FPR')\n",
    "        ax.set_ylabel('TPR')\n",
    "        ax.legend(loc='lower center')        \n",
    "        ax.set_title(f'ROC Curve')\n",
    "        \n",
    "        # PRC\n",
    "        ax = axs[2]\n",
    "        ax.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, marker='X', markersize=7)\n",
    "        ax.set_xlim([-0.02, 1.02])    \n",
    "        ax.set_ylim([-0.02, 1.02])\n",
    "        ax.set_xlabel('recall')\n",
    "        ax.set_ylabel('precision')\n",
    "        ax.legend(loc='lower center')\n",
    "        ax.set_title(f'PRC')        \n",
    "\n",
    "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
    "    \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "    \n",
    "    print(df_eval_stats)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume all models below accepts texts in lowercase and without any digits, punctuations marks etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text(text):\n",
    "    pattern = r\"[^a-zA-Z']\"\n",
    "    text= re.sub(pattern,\" \",text)\n",
    "    text = text.split()\n",
    "    text= \" \".join(text)\n",
    "    return text\n",
    "df_reviews['review_norm'] = text(str(df_reviews['review']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, the whole dataset is already divided into train/test one parts. The corresponding flag is 'ds_part'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tr = df_reviews.query('ds_part == \"train\"').copy()\n",
    "df_test = df_reviews.query('ds_part == \"test\"').copy()\n",
    "\n",
    "train = df_tr['review_norm']\n",
    "test = df_test['review_norm']\n",
    "\n",
    "train_target = df_tr['pos']\n",
    "test_target = df_test['pos']\n",
    "\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 - Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uno=DummyClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uno.fit(train,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_uno,train,train_target,test,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This Dummy model is a baseline for the upcoming models. At this model the F1 score is low, not reaching the 0.85 minimum. \n",
    "Accuracy is also low, at 0.5. APS at 0.5 is low, hence precision and recall is low. At 0.5 the ROC AUC curve cannot distinguish \n",
    "between a positive and negative class. Any lower than this then reciprocation could occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - NLTK, TF-IDF and LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer  = WordNetLemmatizer() \n",
    "\n",
    "def lemmatize(tokens):\n",
    "    lemmas = []\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens] \n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "train_toke = train.apply(word_tokenize)\n",
    "train_lem = train_toke.apply(lemmatize)\n",
    "\n",
    "test_toke = test.apply(word_tokenize)\n",
    "test_lem= test_toke.apply(lemmatize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "vector = TfidfVectorizer(stop_words=stop_words)\n",
    " \n",
    "tf_idf =vector.fit_transform(train_lem) \n",
    "tf_idf_test = vector.transform(test_lem) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1=  LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_1.fit(tf_idf,train_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_1, tf_idf,train_target,tf_idf_test,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "For predicition and analysis the reviews need to be lemmatized and tokenized as shown by the word_tokenize() and WordNetLemmatizer() functions. This is to turn the words into their pure forms. For a more faster more accurate\n",
    "predicion stop_words() function is used to review english stopwords. The Tfidf vectorizer function is used to express the \n",
    "importance of words in the corpus by using a weighting factor.\n",
    "\n",
    "For this first model Logsitic Regression is used. Once evaluated this model outperforms the dummy model. The training set\n",
    "outperforms the test set on all metrics but this to be be expected. In the test set the accuracy is good at 0.88. The F1 metric exceeds the task minimum at 0.88. APS is very high at 0.95 showing the precision and recall is high. The ROC AUCis also at 0.95 showing that the model is very good at class separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3 - spaCy, TF-IDF and LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing_3(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    #tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "train_lemm = train.apply(text_preprocessing_3)\n",
    "test_lemm = test.apply(text_preprocessing_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_3 = TfidfVectorizer(stop_words=stop_words)\n",
    "train_vector =vector_3.fit_transform(train_lemm) \n",
    "test_vector = vector_3.transform(test_lemm) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3= LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_3.fit(train_vector,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_3, train_vector,train_target,test_vector,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "For the spaCy package, nlp is used to preprocess the text for the model. As the first model the tfidf vectorizer is also used, as is Logisitic Regression for the model. Compared to the first model this model results in lower scores. The accuracy and F1 scores are 0.85, below model 1 and also the minimum score the F1 can be for the task. The APS and ROC AUC metric are both at 0.93, lower than model 1. However, these figures being lower could be a positive sign of no model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4 - spaCy, TF-IDF and LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4= LGBMClassifier(random_state=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4.fit(train_vector,train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_4, train_vector,train_target,test_vector,test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Following the preprocessing from the previous model (second model) this model uses LGBM Classifier. As a result the accuracy and F1 score is lower at 0.87 however it still exceeds the minimum F1 score. The APS and ROC AUC scores are still high at 0.94 and 0.95 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model 9 - BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "max_sample_size=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BERT_text_to_embeddings(texts, max_length=512, batch_size=25, force_device=None, disable_progress_bar=False):\n",
    "    \n",
    "    ids_list = []\n",
    "    attention_mask_list = []\n",
    "    \n",
    "    # text to padded ids of tokens along with their attention masks\n",
    "    \n",
    "    # <put your code here to create ids_list and attention_mask_list>\n",
    "    for inputs in texts.iloc[:max_sample_size]:\n",
    "        ids = tokenizer.encode(inputs, add_special_tokens=True, truncation=True, max_length=max_length)\n",
    "        padded = np.array(ids + [0]*(max_length - len(ids)))\n",
    "        attention_mask= np.where(padded != 0,1,0)\n",
    "        ids_list.append(padded)\n",
    "        attention_mask_list.append(attention_mask)\n",
    "    \n",
    "    if force_device is not None:\n",
    "        device = torch.device(force_device)\n",
    "    else:\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "    model.to(device)\n",
    "    if not disable_progress_bar:\n",
    "        print(f'Using the {device} device.')\n",
    "    \n",
    "    # gettings embeddings in batches\n",
    "\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar):\n",
    "            \n",
    "        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        # <put your code here to create attention_mask_batch\n",
    "        attention_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "            \n",
    "        with torch.no_grad():            \n",
    "            model.eval()\n",
    "            batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_batch)   \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
    "        \n",
    "    return np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention! Running BERT for thousands of texts may take long run on CPU, at least several hours\n",
    "train_features_9 = BERT_text_to_embeddings(train, force_device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_9=train_target[:max_sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "print(train_features_9.shape)\n",
    "print(train_target_9.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_9 = BERT_text_to_embeddings(test, force_device='cuda')\n",
    "test_target_9= test_target[:max_sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "print(test_features_9.shape)\n",
    "print(test_target_9.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have got the embeddings, it's advisable to save them to have them ready if \n",
    "np.savez_compressed('features_9.npz', train_features_9=train_features_9, test_features_9=test_features_9)\n",
    "\n",
    "# and load...\n",
    "with np.load('features_9.npz') as data:\n",
    "     train_features_9 = data['train_features_9']\n",
    "     test_features_9 = data['test_features_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_9=LogisticRegression(random_state=12345, solver='liblinear')\n",
    "model_9.fit(train_vectors,train_target_9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_9, train_features_9,train_target_9,test_features_9,test_target_9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This last model uses BERT for preprocessing. This requires making an id_list and an attention mask list from the corpus and turining them into Tensor format. This batch is then embedded. The batch size is 25 and the max sample size is 200 in order to save memory and time. This is applied to the training and test set. Similarly to model 1 and model 2 Logistic regression model is used. With BERT preprocessing the resulting figures are the lowest out of the models, excluding the dummy set. Accuracy is lower at 0.82 and F1 at 0.81. The F1 score does not fufill the task minimum. APS and ROC AUC are higher at 0.94 and 0.93 respectfully, showing that the model is precise and has good class seperability. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_reviews = pd.DataFrame([\n",
    "    'I did not simply like it, not my kind of movie.',\n",
    "    'Well, I was bored and felt asleep in the middle of the movie.',\n",
    "    'I was really fascinated with the movie',    \n",
    "    'Even the actors looked really old and disinterested, and they got paid to be in the movie. What a soulless cash grab.',\n",
    "    'I didn\\'t expect the reboot to be so good! Writers really cared about the source material',\n",
    "    'The movie had its upsides and downsides, but I feel like overall it\\'s a decent flick. I could see myself going to see it again.',\n",
    "    'What a rotten attempt at a comedy. Not a single joke lands, everyone acts annoying and loud, even kids won\\'t like this!',\n",
    "    'Launching on Netflix was a brave move & I really appreciate being able to binge on episode after episode, of this exciting intelligent new drama.'\n",
    "], columns=['review'])\n",
    "\n",
    "my_reviews['review_norm'] = my_reviews['review'].str.lower()\n",
    "\n",
    "my_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_1.predict_proba(vector.transform(texts))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_pred_prob = model_3.predict_proba(vector_3.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "\n",
    "my_reviews_pred_prob = model_4.predict_proba(vector_3.transform(texts.apply(lambda x: text_preprocessing_3(x))))[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = my_reviews['review_norm']\n",
    "\n",
    "my_reviews_features_9 = BERT_text_to_embeddings(texts, disable_progress_bar=True)\n",
    "\n",
    "my_reviews_pred_prob = model_9.predict_proba(my_reviews_features_9)[:, 1]\n",
    "\n",
    "for i, review in enumerate(texts.str.slice(0, 100)):\n",
    "    print(f'{my_reviews_pred_prob[i]:.2f}:  {review}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusion\n",
    "To conclude Model 1 with NLTK as the text processor provides the best results for prediction. The accuracy and F1 is high at 0.88 for the test set. Also the APS and ROC AUC metric at 0.95 concurrs with the other models with being the highest figure. Additionally, the minimum for the F1 score to be above 0.85 is fufilled."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 268,
    "start_time": "2021-07-15T19:42:38.942Z"
   },
   {
    "duration": 231,
    "start_time": "2021-07-15T19:42:41.537Z"
   },
   {
    "duration": 987,
    "start_time": "2021-07-15T19:42:47.640Z"
   },
   {
    "duration": 18,
    "start_time": "2021-07-15T19:42:48.628Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-15T19:42:48.648Z"
   },
   {
    "duration": 3268,
    "start_time": "2021-07-15T19:42:51.221Z"
   },
   {
    "duration": 26,
    "start_time": "2021-07-15T19:42:54.491Z"
   },
   {
    "duration": 27,
    "start_time": "2021-07-15T19:43:16.259Z"
   },
   {
    "duration": 26,
    "start_time": "2021-07-15T19:43:36.038Z"
   },
   {
    "duration": 62,
    "start_time": "2021-07-15T19:43:54.478Z"
   },
   {
    "duration": 76,
    "start_time": "2021-07-15T19:44:42.084Z"
   },
   {
    "duration": 75,
    "start_time": "2021-07-15T19:45:24.138Z"
   },
   {
    "duration": 64,
    "start_time": "2021-07-15T19:45:46.250Z"
   },
   {
    "duration": 51,
    "start_time": "2021-07-15T19:45:54.554Z"
   },
   {
    "duration": 81,
    "start_time": "2021-07-15T19:46:02.804Z"
   },
   {
    "duration": 265,
    "start_time": "2021-07-15T20:02:22.043Z"
   },
   {
    "duration": 35,
    "start_time": "2021-07-15T20:02:29.756Z"
   },
   {
    "duration": 5718,
    "start_time": "2021-07-15T20:02:32.321Z"
   },
   {
    "duration": -2453,
    "start_time": "2021-07-15T20:02:40.494Z"
   },
   {
    "duration": 590,
    "start_time": "2021-07-15T20:02:49.931Z"
   },
   {
    "duration": 80,
    "start_time": "2021-07-15T20:02:50.522Z"
   },
   {
    "duration": 73,
    "start_time": "2021-07-15T20:02:50.604Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-15T20:02:50.749Z"
   },
   {
    "duration": 4659,
    "start_time": "2021-07-15T20:02:51.824Z"
   },
   {
    "duration": 1016,
    "start_time": "2021-07-15T20:02:56.485Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-15T20:02:57.503Z"
   },
   {
    "duration": 10,
    "start_time": "2021-07-15T20:02:57.507Z"
   },
   {
    "duration": 650,
    "start_time": "2021-07-15T20:02:57.519Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-15T20:02:58.170Z"
   },
   {
    "duration": 5778,
    "start_time": "2021-07-15T20:02:58.174Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-15T20:03:03.954Z"
   },
   {
    "duration": 21,
    "start_time": "2021-07-15T20:03:03.958Z"
   },
   {
    "duration": 5525,
    "start_time": "2021-07-15T20:03:03.980Z"
   },
   {
    "duration": 124,
    "start_time": "2021-07-15T20:03:57.509Z"
   },
   {
    "duration": 46,
    "start_time": "2021-07-15T20:03:59.124Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-15T20:04:00.856Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-15T20:04:02.121Z"
   },
   {
    "duration": 241,
    "start_time": "2021-07-15T20:04:04.247Z"
   },
   {
    "duration": 2051,
    "start_time": "2021-07-15T20:04:12.699Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-15T20:04:24.115Z"
   },
   {
    "duration": 1597,
    "start_time": "2021-07-15T20:04:24.681Z"
   },
   {
    "duration": 138,
    "start_time": "2021-07-15T20:13:09.341Z"
   },
   {
    "duration": 245,
    "start_time": "2021-07-15T20:13:09.799Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-15T20:13:16.922Z"
   },
   {
    "duration": 299,
    "start_time": "2021-07-15T20:13:17.673Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-15T20:13:25.739Z"
   },
   {
    "duration": 232,
    "start_time": "2021-07-15T20:13:27.560Z"
   },
   {
    "duration": 253,
    "start_time": "2021-07-15T20:13:35.755Z"
   },
   {
    "duration": 230,
    "start_time": "2021-07-15T20:13:59.624Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-15T20:19:31.681Z"
   },
   {
    "duration": 233,
    "start_time": "2021-07-15T20:19:32.503Z"
   },
   {
    "duration": 239,
    "start_time": "2021-07-15T20:21:06.716Z"
   },
   {
    "duration": 220,
    "start_time": "2021-07-15T20:22:24.851Z"
   },
   {
    "duration": 228,
    "start_time": "2021-07-15T20:22:31.017Z"
   },
   {
    "duration": 242,
    "start_time": "2021-07-15T20:24:18.706Z"
   },
   {
    "duration": 225,
    "start_time": "2021-07-15T20:25:04.128Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-15T20:30:47.021Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-15T20:30:52.756Z"
   },
   {
    "duration": 236,
    "start_time": "2021-07-15T20:30:53.372Z"
   },
   {
    "duration": 1024,
    "start_time": "2021-07-17T09:49:23.897Z"
   },
   {
    "duration": 17,
    "start_time": "2021-07-17T09:49:24.923Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-17T09:49:24.941Z"
   },
   {
    "duration": 584,
    "start_time": "2021-07-17T09:49:25.429Z"
   },
   {
    "duration": 77,
    "start_time": "2021-07-17T09:49:26.015Z"
   },
   {
    "duration": 86,
    "start_time": "2021-07-17T09:49:26.325Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T09:49:26.675Z"
   },
   {
    "duration": 4489,
    "start_time": "2021-07-17T09:49:27.789Z"
   },
   {
    "duration": 1020,
    "start_time": "2021-07-17T09:49:32.280Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T09:49:33.302Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-17T09:49:33.306Z"
   },
   {
    "duration": 749,
    "start_time": "2021-07-17T09:49:33.315Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T09:49:34.066Z"
   },
   {
    "duration": 6195,
    "start_time": "2021-07-17T09:49:34.069Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T09:49:40.266Z"
   },
   {
    "duration": 49,
    "start_time": "2021-07-17T09:49:40.269Z"
   },
   {
    "duration": 123,
    "start_time": "2021-07-17T09:49:40.319Z"
   },
   {
    "duration": 42,
    "start_time": "2021-07-17T09:49:40.443Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-17T09:49:40.491Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-17T09:49:40.496Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-17T09:49:40.501Z"
   },
   {
    "duration": 1329,
    "start_time": "2021-07-17T09:49:40.509Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T09:49:42.777Z"
   },
   {
    "duration": 230,
    "start_time": "2021-07-17T09:49:46.332Z"
   },
   {
    "duration": 356,
    "start_time": "2021-07-17T09:49:47.511Z"
   },
   {
    "duration": 233,
    "start_time": "2021-07-17T09:50:46.925Z"
   },
   {
    "duration": 232,
    "start_time": "2021-07-17T09:51:47.084Z"
   },
   {
    "duration": 227,
    "start_time": "2021-07-17T09:52:00.301Z"
   },
   {
    "duration": 365,
    "start_time": "2021-07-17T09:52:58.767Z"
   },
   {
    "duration": 395,
    "start_time": "2021-07-17T09:53:09.503Z"
   },
   {
    "duration": 106,
    "start_time": "2021-07-17T09:53:32.347Z"
   },
   {
    "duration": 107,
    "start_time": "2021-07-17T09:54:40.870Z"
   },
   {
    "duration": 112,
    "start_time": "2021-07-17T09:55:17.853Z"
   },
   {
    "duration": 114,
    "start_time": "2021-07-17T09:56:57.023Z"
   },
   {
    "duration": 104,
    "start_time": "2021-07-17T09:58:25.593Z"
   },
   {
    "duration": 29,
    "start_time": "2021-07-17T09:58:27.720Z"
   },
   {
    "duration": 31,
    "start_time": "2021-07-17T09:59:01.123Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T09:59:06.357Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-17T09:59:06.718Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-17T09:59:06.989Z"
   },
   {
    "duration": 1292,
    "start_time": "2021-07-17T09:59:07.269Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-17T09:59:21.689Z"
   },
   {
    "duration": 228,
    "start_time": "2021-07-17T09:59:29.685Z"
   },
   {
    "duration": 5913,
    "start_time": "2021-07-17T09:59:41.353Z"
   },
   {
    "duration": 7309,
    "start_time": "2021-07-17T09:59:47.591Z"
   },
   {
    "duration": 231,
    "start_time": "2021-07-17T10:00:06.424Z"
   },
   {
    "duration": 1294,
    "start_time": "2021-07-17T10:00:16.055Z"
   },
   {
    "duration": 764,
    "start_time": "2021-07-17T10:01:41.491Z"
   },
   {
    "duration": 108,
    "start_time": "2021-07-17T10:01:55.858Z"
   },
   {
    "duration": 39,
    "start_time": "2021-07-17T10:01:58.157Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-17T10:01:59.577Z"
   },
   {
    "duration": 2,
    "start_time": "2021-07-17T10:02:00.071Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-17T10:02:00.427Z"
   },
   {
    "duration": 1322,
    "start_time": "2021-07-17T10:02:00.760Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-17T10:02:02.084Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-17T10:02:02.373Z"
   },
   {
    "duration": 238,
    "start_time": "2021-07-17T10:02:03.225Z"
   },
   {
    "duration": 238,
    "start_time": "2021-07-17T10:02:39.863Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-17T10:02:47.243Z"
   },
   {
    "duration": 232,
    "start_time": "2021-07-17T10:02:59.063Z"
   },
   {
    "duration": -5054,
    "start_time": "2021-07-17T10:05:41.339Z"
   },
   {
    "duration": -5056,
    "start_time": "2021-07-17T10:05:41.342Z"
   },
   {
    "duration": -5056,
    "start_time": "2021-07-17T10:05:41.344Z"
   },
   {
    "duration": -5058,
    "start_time": "2021-07-17T10:05:41.347Z"
   },
   {
    "duration": 96395,
    "start_time": "2021-07-17T10:05:54.885Z"
   },
   {
    "duration": 6327,
    "start_time": "2021-07-17T10:07:31.282Z"
   },
   {
    "duration": 6678,
    "start_time": "2021-07-17T10:08:13.316Z"
   },
   {
    "duration": 1350,
    "start_time": "2021-07-17T10:08:19.996Z"
   },
   {
    "duration": 1025,
    "start_time": "2021-07-17T10:12:28.183Z"
   },
   {
    "duration": 1774,
    "start_time": "2021-07-17T10:12:30.571Z"
   },
   {
    "duration": 335542,
    "start_time": "2021-07-17T10:15:34.814Z"
   },
   {
    "duration": 446,
    "start_time": "2021-07-17T10:23:35.011Z"
   },
   {
    "duration": 1458,
    "start_time": "2021-07-21T14:43:21.981Z"
   },
   {
    "duration": 34,
    "start_time": "2021-07-21T14:43:24.535Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-21T14:43:24.920Z"
   },
   {
    "duration": 1466,
    "start_time": "2021-07-21T14:43:39.019Z"
   },
   {
    "duration": 148,
    "start_time": "2021-07-21T14:43:40.489Z"
   },
   {
    "duration": 121,
    "start_time": "2021-07-21T14:43:40.641Z"
   },
   {
    "duration": 8853,
    "start_time": "2021-07-21T14:43:43.077Z"
   },
   {
    "duration": 1905,
    "start_time": "2021-07-21T14:43:51.934Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-21T14:43:53.842Z"
   },
   {
    "duration": 1164,
    "start_time": "2021-07-21T14:43:53.852Z"
   },
   {
    "duration": 11043,
    "start_time": "2021-07-21T14:43:55.018Z"
   },
   {
    "duration": 75,
    "start_time": "2021-07-21T14:44:06.064Z"
   },
   {
    "duration": 698,
    "start_time": "2021-07-21T14:44:06.142Z"
   },
   {
    "duration": 812,
    "start_time": "2021-07-21T14:45:04.229Z"
   },
   {
    "duration": 744,
    "start_time": "2021-07-21T14:45:17.075Z"
   },
   {
    "duration": 377,
    "start_time": "2021-07-21T14:46:40.067Z"
   },
   {
    "duration": 513,
    "start_time": "2021-07-21T14:47:29.053Z"
   },
   {
    "duration": 210,
    "start_time": "2021-07-21T14:48:57.827Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-21T14:49:11.055Z"
   },
   {
    "duration": 504,
    "start_time": "2021-07-21T14:50:01.995Z"
   },
   {
    "duration": 182,
    "start_time": "2021-07-21T14:50:16.367Z"
   },
   {
    "duration": 159,
    "start_time": "2021-07-21T14:50:27.423Z"
   },
   {
    "duration": 55,
    "start_time": "2021-07-21T14:50:30.296Z"
   },
   {
    "duration": 6,
    "start_time": "2021-07-21T14:50:36.172Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-21T14:50:36.538Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-21T14:50:36.925Z"
   },
   {
    "duration": 2196,
    "start_time": "2021-07-21T14:50:37.586Z"
   },
   {
    "duration": 358,
    "start_time": "2021-07-21T14:50:49.409Z"
   },
   {
    "duration": 60989,
    "start_time": "2021-07-21T14:50:50.523Z"
   },
   {
    "duration": 3739,
    "start_time": "2021-07-21T14:51:51.515Z"
   },
   {
    "duration": 65,
    "start_time": "2021-07-21T14:51:55.257Z"
   },
   {
    "duration": 2346,
    "start_time": "2021-07-21T14:51:55.325Z"
   },
   {
    "duration": 361,
    "start_time": "2021-07-21T14:53:16.587Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-21T14:53:28.920Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-21T14:53:39.268Z"
   },
   {
    "duration": 37,
    "start_time": "2021-07-21T14:53:41.025Z"
   },
   {
    "duration": 11,
    "start_time": "2021-07-21T14:53:43.183Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-21T14:53:43.446Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-21T14:53:43.751Z"
   },
   {
    "duration": 2641,
    "start_time": "2021-07-21T14:53:44.023Z"
   },
   {
    "duration": 5,
    "start_time": "2021-07-21T14:53:50.861Z"
   },
   {
    "duration": 59609,
    "start_time": "2021-07-21T14:53:51.273Z"
   },
   {
    "duration": 3679,
    "start_time": "2021-07-21T14:54:50.886Z"
   },
   {
    "duration": 58,
    "start_time": "2021-07-21T14:54:54.568Z"
   },
   {
    "duration": 2292,
    "start_time": "2021-07-21T14:54:54.628Z"
   },
   {
    "duration": 20,
    "start_time": "2021-07-21T14:55:16.654Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-21T14:55:17.046Z"
   },
   {
    "duration": 397,
    "start_time": "2021-07-21T14:55:17.490Z"
   },
   {
    "duration": 361,
    "start_time": "2021-07-21T14:55:18.296Z"
   },
   {
    "duration": 3037,
    "start_time": "2021-07-21T14:55:23.321Z"
   },
   {
    "duration": 297167,
    "start_time": "2021-07-21T14:55:26.361Z"
   },
   {
    "duration": 2407,
    "start_time": "2021-07-21T15:00:23.531Z"
   },
   {
    "duration": 59,
    "start_time": "2021-07-21T15:00:25.941Z"
   },
   {
    "duration": 2552,
    "start_time": "2021-07-21T15:00:26.003Z"
   },
   {
    "duration": 237,
    "start_time": "2021-07-21T15:01:51.451Z"
   },
   {
    "duration": 2217,
    "start_time": "2021-07-21T15:01:52.817Z"
   },
   {
    "duration": 355,
    "start_time": "2021-07-21T15:02:16.663Z"
   },
   {
    "duration": 539,
    "start_time": "2021-07-21T15:02:27.549Z"
   },
   {
    "duration": 582,
    "start_time": "2021-07-21T15:03:56.067Z"
   },
   {
    "duration": 794,
    "start_time": "2021-07-21T15:04:12.021Z"
   },
   {
    "duration": 339,
    "start_time": "2021-07-21T15:04:18.945Z"
   },
   {
    "duration": 576,
    "start_time": "2021-07-21T15:04:24.521Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-21T15:04:51.413Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-21T15:05:09.755Z"
   },
   {
    "duration": 54,
    "start_time": "2021-07-21T15:05:11.538Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-21T15:05:13.317Z"
   },
   {
    "duration": 3,
    "start_time": "2021-07-21T15:05:13.691Z"
   },
   {
    "duration": 7,
    "start_time": "2021-07-21T15:05:14.015Z"
   },
   {
    "duration": 2178,
    "start_time": "2021-07-21T15:05:14.311Z"
   },
   {
    "duration": 4,
    "start_time": "2021-07-21T15:05:19.133Z"
   },
   {
    "duration": 47052,
    "start_time": "2021-07-21T15:05:19.601Z"
   },
   {
    "duration": 3306,
    "start_time": "2021-07-21T15:06:06.656Z"
   },
   {
    "duration": 58,
    "start_time": "2021-07-21T15:06:09.965Z"
   },
   {
    "duration": 2160,
    "start_time": "2021-07-21T15:06:10.026Z"
   },
   {
    "duration": 401,
    "start_time": "2021-07-21T15:07:39.892Z"
   },
   {
    "duration": 14,
    "start_time": "2021-07-21T15:07:49.145Z"
   },
   {
    "duration": 8,
    "start_time": "2021-07-21T15:07:50.413Z"
   },
   {
    "duration": 376,
    "start_time": "2021-07-21T15:07:52.729Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
